{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        # Dynamically calculate the flattened size\n",
    "        self.flatten_size = None\n",
    "        self.fc1 = nn.Linear(1, 64)  # Placeholder, will be initialized in forward\n",
    "        self.fc_mean = nn.Linear(64, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(64, latent_dim)\n",
    "        self.initialized = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.initialized:\n",
    "            self.flatten_size = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "            self.fc1 = nn.Linear(self.flatten_size, 64)  # Proper initialization\n",
    "            self.initialized = True\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mean, log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Assuming the latent dimension reshapes to something that can be scaled up to the input dimensions\n",
    "        self.fc = nn.Linear(latent_dim, 128 * 17 * 1)  # Adjust the multiplication factors to fit your specific structure\n",
    "        self.conv2d_t1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv2d_t2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv2d_t3 = nn.ConvTranspose2d(32, output_channels, kernel_size=(1,1), stride=2, padding=(51,2), output_padding=(0,1))\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc(z))\n",
    "        z = z.view(-1, 128, 17, 1)  # Adjusted to match the output of the encoder's last conv layer\n",
    "        z = F.relu(self.conv2d_t1(z))\n",
    "        z = F.relu(self.conv2d_t2(z))\n",
    "        reconstruction = torch.sigmoid(self.conv2d_t3(z))\n",
    "        return reconstruction\n",
    "\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, input_channels, latent_dim, output_channels):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.encoder = Encoder(input_channels, latent_dim)\n",
    "#         self.decoder = Decoder(latent_dim, output_channels)\n",
    "\n",
    "#     def reparameterize(self, mean, log_var):\n",
    "#         std = torch.exp(0.5 * log_var)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mean + eps * std\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean, log_var = self.encoder(x)\n",
    "#         z = self.reparameterize(mean, log_var)\n",
    "#         return self.decoder(z), mean, log_var\n",
    "    \n",
    "class SupervisedVAE(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim, output_channels, num_classes=11):\n",
    "        super(SupervisedVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, output_channels)\n",
    "        self.classifier = nn.Linear(latent_dim, num_classes)  # Classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        reconstruction = self.decoder(z)\n",
    "        class_logits = self.classifier(z)\n",
    "        return reconstruction, mean, log_var, class_logits\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-07-04 12:04:17.738369</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.367</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-07-04 12:04:17.790509</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.367</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-07-04 12:04:17.836632</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.373</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-07-04 12:04:17.885262</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-07-04 12:04:17.945423</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294673</th>\n",
       "      <td>2018-07-12 12:18:28.783680</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.989</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294674</th>\n",
       "      <td>2018-07-12 12:18:28.832811</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.998</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294675</th>\n",
       "      <td>2018-07-12 12:18:28.892470</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.998</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294676</th>\n",
       "      <td>2018-07-12 12:18:29.025324</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.992</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294677</th>\n",
       "      <td>2018-07-12 12:18:29.070446</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.992</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294678 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp  acc_x  acc_y  acc_z  activity\n",
       "0      2018-07-04 12:04:17.738369  0.146  0.895  0.367         1\n",
       "1      2018-07-04 12:04:17.790509  0.146  0.895  0.367         1\n",
       "2      2018-07-04 12:04:17.836632  0.178  0.896  0.373         1\n",
       "3      2018-07-04 12:04:17.885262  0.160  0.895  0.372         1\n",
       "4      2018-07-04 12:04:17.945423  0.160  0.895  0.372         1\n",
       "...                           ...    ...    ...    ...       ...\n",
       "294673 2018-07-12 12:18:28.783680 -0.377 -0.095  0.989        11\n",
       "294674 2018-07-12 12:18:28.832811 -0.376 -0.090  0.998        11\n",
       "294675 2018-07-12 12:18:28.892470 -0.376 -0.090  0.998        11\n",
       "294676 2018-07-12 12:18:29.025324 -0.381 -0.101  0.992        11\n",
       "294677 2018-07-12 12:18:29.070446 -0.381 -0.101  0.992        11\n",
       "\n",
       "[294678 rows x 5 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import stft\n",
    "\n",
    "data_path = '../datasets/UPFallCompleteDataSet.csv'\n",
    "data = pd.read_csv(data_path, skiprows=1, usecols=[0, 15, 16, 17, 44])  # Adjust column indices as needed\n",
    "\n",
    "# Rename columns for clarity\n",
    "data.columns = ['timestamp', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "# Drop rows with any NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def create_overlapping_windows_with_labels(signals, labels, window_size, overlap_size):\n",
    "    step_size = window_size - overlap_size\n",
    "    windowed_signals = []\n",
    "    windowed_labels = []\n",
    "    total_windows = 0\n",
    "\n",
    "    for start in range(0, len(signals[0]) - window_size + 1, step_size):  # Assumes all signals are the same length\n",
    "        end = start + window_size\n",
    "        window = [signal[start:end] for signal in signals]\n",
    "        label_window = labels[start:end]\n",
    "        if len(window[0]) == window_size and len(label_window) == window_size:\n",
    "            windowed_signals.append(window)\n",
    "            # Using mode of labels in the window as the window label\n",
    "            windowed_label = mode(label_window)[0]\n",
    "            windowed_labels.append(windowed_label)\n",
    "            total_windows += 1\n",
    "        else:\n",
    "            print(f\"Skipped window from {start} to {end} due to incorrect size.\")\n",
    "\n",
    "    return np.array(windowed_signals), np.array(windowed_labels)\n",
    "\n",
    "def generate_multi_channel_spectrogram(windows, fs, window_type, nperseg, noverlap, nfft):\n",
    "    spectrograms = []\n",
    "    for window in windows:\n",
    "        # Assuming window is a 2D array with shape (window_size, num_channels)\n",
    "        window_spectrograms = []\n",
    "        for signal in window:  # Transpose to iterate over channels\n",
    "            _, _, Zxx = stft(signal, fs=fs, window=window_type, nperseg=nperseg, noverlap=noverlap, nfft=nfft)\n",
    "            spectrogram = np.log(np.abs(Zxx) + 1e-8)  # Convert to dB scale\n",
    "            # Normalize the spectrogram\n",
    "            mean = np.mean(spectrogram)\n",
    "            std = np.std(spectrogram)\n",
    "            spectrogram = (spectrogram - mean) / std\n",
    "            window_spectrograms.append(spectrogram)\n",
    "        # Stack to form a multi-channel spectrogram for this window\n",
    "        spectrograms.append(np.stack(window_spectrograms, axis=0))\n",
    "    return np.array(spectrograms)\n",
    "\n",
    "def plot_simple_spectrogram(spectrogram, channel=0, title=\"Spectrogram\"):\n",
    "    # spectrogram should be a 3D array: [frequency, time, channel]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    # Display the spectrogram of a specific channel\n",
    "    plt.imshow(spectrogram[:, :, channel].T, aspect='auto', origin='lower')\n",
    "    plt.colorbar(label='Intensity (dB)')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Sample Index')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example parameters\n",
    "window_size = 45 # aroung 4sec of data\n",
    "overlap_size = 22\n",
    "fs = 15  # Sampling frequency is aroung 12.85\n",
    "\n",
    "signals = [data['acc_x'].values, data['acc_y'].values, data['acc_z'].values]\n",
    "activities = data['activity'].values - 1 # make it zero-indexed\n",
    "\n",
    "# Create overlapping windows for each signal and stack them to form multi-dimensional windows\n",
    "windowed_signals, windowed_labels = create_overlapping_windows_with_labels(signals, activities, window_size, overlap_size)\n",
    "\n",
    "# Generate spectrograms for each window\n",
    "window_type='hann'\n",
    "nperseg=32\n",
    "noverlap=16\n",
    "nfft=64\n",
    "spectrograms = generate_multi_channel_spectrogram(windowed_signals, fs, window_type, nperseg, noverlap, nfft)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectrograms, windowed_labels, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "                            Default: 5\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.epochs_no_improve = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.epochs_no_improve += 1\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.epochs_no_improve = 0\n",
    "\n",
    "        if self.verbose and self.epochs_no_improve == 0:\n",
    "            print(f'Validation loss decreased ({self.best_score - score:.6f} --> {score:.6f}).  Saving model ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "  def __init__(self, X, y, device='cpu'):\n",
    "    self.x_data = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    self.y_data = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    seq_in = self.x_data[idx]\n",
    "    target = self.y_data[idx]\n",
    "    sample = { 'spectrograms' : seq_in, 'label' : target }\n",
    "    return sample\n",
    "\n",
    "# Convert to PyTorch tensors and create DataLoaders\n",
    "train_data = Data(X_train, y_train)\n",
    "test_data = Data(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Loss function\n",
    "def supervised_vae_loss(recon_x, x, mean, log_var, logits, labels):\n",
    "    # print(f\"Recon_x size: {recon_x.size()}, x size: {x.size()}\")\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    labels = labels.long()\n",
    "    CE = F.cross_entropy(logits, labels)\n",
    "    return MSE, KLD, CE, MSE/10000 + KLD + CE\n",
    "\n",
    "# Train Function\n",
    "def train_supervised_vae(model, data_loader, epochs, learning_rate=1e-3, device='cpu'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_MSE = 0\n",
    "        total_KLD = 0\n",
    "        total_CE = 0\n",
    "        for batch in data_loader:\n",
    "            data = batch['spectrograms'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mean, log_var, class_logits = model(data)\n",
    "            MSE, KLD, CE, loss = supervised_vae_loss(recon_batch, data, mean, log_var, class_logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss\n",
    "            total_MSE += MSE\n",
    "            total_CE += CE\n",
    "            total_KLD += KLD\n",
    "        \n",
    "        # scheduler.step()\n",
    "        average_loss = total_loss / len(data_loader)\n",
    "        average_MSE = total_MSE / len(data_loader)\n",
    "        average_KLD = total_KLD / len(data_loader)\n",
    "        average_CE = total_CE / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss:.4f}, Average KLD: {average_KLD:.8f}, Weighted Average MSE: {average_MSE:.4f} / 10000 = {average_MSE/10000:.4f}, Average CE: {average_CE:.4f}')\n",
    "        #  / 1.5 = {average_KLD/1.5:.4f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# # Loss function\n",
    "# def supervised_vae_loss(recon_x, x, mean, log_var, logits, labels):\n",
    "#     print(f\"Recon_x size: {recon_x.size()}, x size: {x.size()}\")\n",
    "#     print(recon_x, x)\n",
    "#     MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "#     KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "#     labels = labels.long()\n",
    "#     CE = F.cross_entropy(logits, labels)\n",
    "#     return MSE + KLD + CE\n",
    "\n",
    "# # Train Function\n",
    "# def train_supervised_vae(model, data_loader, val_loader, epochs, learning_rate=1e-3, device='cuda'):\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#     early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "#     min_loss = 100000000\n",
    "#     current_loss = 0\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch in data_loader:\n",
    "#             data = batch['spectrograms'].to(device)\n",
    "#             labels = batch['label'].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             recon_batch, mean, log_var, class_logits = model(data)\n",
    "#             current_loss = supervised_vae_loss(recon_batch, data, mean, log_var, class_logits, labels)\n",
    "#             current_loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += current_loss.item()\n",
    "#             if current_loss < min_loss:\n",
    "#                 min_loss = current_loss\n",
    "        \n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0\n",
    "#         current_val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 data = batch['spectrograms'].to(device)\n",
    "#                 labels = batch['label'].to(device)\n",
    "#                 recon_batch, mean, log_var, class_logits = model(data)\n",
    "#                 current_val_loss = supervised_vae_loss(recon_batch, data, mean, log_var, class_logits, labels)\n",
    "#                 total_val_loss += current_val_loss.item()\n",
    "\n",
    "#         # Scheduler and Early Stopping\n",
    "#         val_loss_avg = total_val_loss / len(val_loader)\n",
    "#         early_stopping(val_loss_avg)\n",
    "#         # scheduler.step()\n",
    "#         average_loss = total_loss / len(data_loader)\n",
    "#         print(f'Epoch {epoch+1}, Average Loss: {average_loss:.4f}, Min Loss: {min_loss:.4f}')\n",
    "        \n",
    "#         if early_stopping.early_stop:\n",
    "#             print(f\"Early stopping. Current Loss: {current_val_loss:.4f}\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 3.8540, Average KLD: 0.07786067, Weighted Average MSE: 13695.3232 / 10000 = 1.3695, Average CE: 2.4066\n",
      "Epoch 2, Average Loss: 3.5642, Average KLD: 0.00131312, Weighted Average MSE: 13057.1846 / 10000 = 1.3057, Average CE: 2.2572\n",
      "Epoch 3, Average Loss: 3.4509, Average KLD: 0.00075580, Weighted Average MSE: 12700.1523 / 10000 = 1.2700, Average CE: 2.1801\n",
      "Epoch 4, Average Loss: 3.3847, Average KLD: 0.00049653, Weighted Average MSE: 12441.8262 / 10000 = 1.2442, Average CE: 2.1400\n",
      "Epoch 5, Average Loss: 3.3463, Average KLD: 0.00037672, Weighted Average MSE: 12254.4453 / 10000 = 1.2254, Average CE: 2.1205\n",
      "Epoch 6, Average Loss: 3.3262, Average KLD: 0.00030121, Weighted Average MSE: 12116.3145 / 10000 = 1.2116, Average CE: 2.1143\n",
      "Epoch 7, Average Loss: 3.3093, Average KLD: 0.00023580, Weighted Average MSE: 12013.2715 / 10000 = 1.2013, Average CE: 2.1078\n",
      "Epoch 8, Average Loss: 3.3014, Average KLD: 0.00023040, Weighted Average MSE: 11935.5146 / 10000 = 1.1936, Average CE: 2.1076\n",
      "Epoch 9, Average Loss: 3.2938, Average KLD: 0.00020443, Weighted Average MSE: 11875.1211 / 10000 = 1.1875, Average CE: 2.1061\n",
      "Epoch 10, Average Loss: 3.2870, Average KLD: 0.00016324, Weighted Average MSE: 11828.0752 / 10000 = 1.1828, Average CE: 2.1041\n",
      "Epoch 11, Average Loss: 3.2843, Average KLD: 0.00014894, Weighted Average MSE: 11790.2402 / 10000 = 1.1790, Average CE: 2.1052\n",
      "Epoch 12, Average Loss: 3.2823, Average KLD: 0.00014875, Weighted Average MSE: 11760.0039 / 10000 = 1.1760, Average CE: 2.1062\n",
      "Epoch 13, Average Loss: 3.2779, Average KLD: 0.00014951, Weighted Average MSE: 11735.1680 / 10000 = 1.1735, Average CE: 2.1042\n",
      "Epoch 14, Average Loss: 3.2772, Average KLD: 0.00014311, Weighted Average MSE: 11714.6924 / 10000 = 1.1715, Average CE: 2.1056\n",
      "Epoch 15, Average Loss: 3.2743, Average KLD: 0.00011842, Weighted Average MSE: 11697.9951 / 10000 = 1.1698, Average CE: 2.1044\n",
      "Epoch 16, Average Loss: 3.2740, Average KLD: 0.00012077, Weighted Average MSE: 11683.3701 / 10000 = 1.1683, Average CE: 2.1055\n",
      "Epoch 17, Average Loss: 3.2708, Average KLD: 0.00009162, Weighted Average MSE: 11671.3682 / 10000 = 1.1671, Average CE: 2.1035\n",
      "Epoch 18, Average Loss: 3.2708, Average KLD: 0.00008082, Weighted Average MSE: 11661.5010 / 10000 = 1.1662, Average CE: 2.1045\n",
      "Epoch 19, Average Loss: 3.2695, Average KLD: 0.00009772, Weighted Average MSE: 11652.1924 / 10000 = 1.1652, Average CE: 2.1042\n",
      "Epoch 20, Average Loss: 3.2696, Average KLD: 0.00007096, Weighted Average MSE: 11644.6670 / 10000 = 1.1645, Average CE: 2.1050\n",
      "Epoch 21, Average Loss: 3.2694, Average KLD: 0.00007359, Weighted Average MSE: 11638.0166 / 10000 = 1.1638, Average CE: 2.1056\n",
      "Epoch 22, Average Loss: 3.2691, Average KLD: 0.00008905, Weighted Average MSE: 11632.2959 / 10000 = 1.1632, Average CE: 2.1058\n",
      "Epoch 23, Average Loss: 3.2676, Average KLD: 0.00007623, Weighted Average MSE: 11627.4023 / 10000 = 1.1627, Average CE: 2.1048\n",
      "Epoch 24, Average Loss: 3.2675, Average KLD: 0.00006798, Weighted Average MSE: 11623.0293 / 10000 = 1.1623, Average CE: 2.1051\n",
      "Epoch 25, Average Loss: 3.2656, Average KLD: 0.00007086, Weighted Average MSE: 11618.9834 / 10000 = 1.1619, Average CE: 2.1036\n",
      "Epoch 26, Average Loss: 3.2664, Average KLD: 0.00006125, Weighted Average MSE: 11615.4766 / 10000 = 1.1615, Average CE: 2.1048\n",
      "Epoch 27, Average Loss: 3.2666, Average KLD: 0.00007648, Weighted Average MSE: 11612.8799 / 10000 = 1.1613, Average CE: 2.1052\n",
      "Epoch 28, Average Loss: 3.2653, Average KLD: 0.00005738, Weighted Average MSE: 11609.9854 / 10000 = 1.1610, Average CE: 2.1042\n",
      "Epoch 29, Average Loss: 3.2652, Average KLD: 0.00004986, Weighted Average MSE: 11607.7090 / 10000 = 1.1608, Average CE: 2.1043\n",
      "Epoch 30, Average Loss: 3.2648, Average KLD: 0.00005523, Weighted Average MSE: 11605.5566 / 10000 = 1.1606, Average CE: 2.1042\n",
      "Epoch 31, Average Loss: 3.2654, Average KLD: 0.00005685, Weighted Average MSE: 11603.7041 / 10000 = 1.1604, Average CE: 2.1050\n",
      "Epoch 32, Average Loss: 3.2661, Average KLD: 0.00004461, Weighted Average MSE: 11602.0928 / 10000 = 1.1602, Average CE: 2.1059\n",
      "Epoch 33, Average Loss: 3.2651, Average KLD: 0.00004298, Weighted Average MSE: 11600.6133 / 10000 = 1.1601, Average CE: 2.1050\n",
      "Epoch 34, Average Loss: 3.2661, Average KLD: 0.00004749, Weighted Average MSE: 11599.3477 / 10000 = 1.1599, Average CE: 2.1062\n",
      "Epoch 35, Average Loss: 3.2643, Average KLD: 0.00004854, Weighted Average MSE: 11597.9648 / 10000 = 1.1598, Average CE: 2.1045\n",
      "Epoch 36, Average Loss: 3.2635, Average KLD: 0.00004753, Weighted Average MSE: 11597.0996 / 10000 = 1.1597, Average CE: 2.1037\n",
      "Epoch 37, Average Loss: 3.2640, Average KLD: 0.00005250, Weighted Average MSE: 11596.3555 / 10000 = 1.1596, Average CE: 2.1043\n",
      "Epoch 38, Average Loss: 3.2631, Average KLD: 0.00005541, Weighted Average MSE: 11595.5029 / 10000 = 1.1596, Average CE: 2.1035\n",
      "Epoch 39, Average Loss: 3.2660, Average KLD: 0.00005301, Weighted Average MSE: 11594.5527 / 10000 = 1.1595, Average CE: 2.1065\n",
      "Epoch 40, Average Loss: 3.2651, Average KLD: 0.00005118, Weighted Average MSE: 11594.3789 / 10000 = 1.1594, Average CE: 2.1056\n",
      "Epoch 41, Average Loss: 3.2648, Average KLD: 0.00004955, Weighted Average MSE: 11593.4941 / 10000 = 1.1593, Average CE: 2.1054\n",
      "Epoch 42, Average Loss: 3.2641, Average KLD: 0.00005319, Weighted Average MSE: 11593.1396 / 10000 = 1.1593, Average CE: 2.1047\n",
      "Epoch 43, Average Loss: 3.2623, Average KLD: 0.00004717, Weighted Average MSE: 11592.7412 / 10000 = 1.1593, Average CE: 2.1030\n",
      "Epoch 44, Average Loss: 3.2626, Average KLD: 0.00005267, Weighted Average MSE: 11592.2178 / 10000 = 1.1592, Average CE: 2.1034\n",
      "Epoch 45, Average Loss: 3.2635, Average KLD: 0.00004640, Weighted Average MSE: 11591.7764 / 10000 = 1.1592, Average CE: 2.1043\n",
      "Epoch 46, Average Loss: 3.2637, Average KLD: 0.00004654, Weighted Average MSE: 11591.6348 / 10000 = 1.1592, Average CE: 2.1045\n",
      "Epoch 47, Average Loss: 3.2620, Average KLD: 0.00005512, Weighted Average MSE: 11591.1943 / 10000 = 1.1591, Average CE: 2.1028\n",
      "Epoch 48, Average Loss: 3.2636, Average KLD: 0.00006543, Weighted Average MSE: 11591.0332 / 10000 = 1.1591, Average CE: 2.1045\n",
      "Epoch 49, Average Loss: 3.2633, Average KLD: 0.00005505, Weighted Average MSE: 11590.7783 / 10000 = 1.1591, Average CE: 2.1042\n",
      "Epoch 50, Average Loss: 3.2634, Average KLD: 0.00005160, Weighted Average MSE: 11590.6953 / 10000 = 1.1591, Average CE: 2.1043\n",
      "Epoch 51, Average Loss: 3.2636, Average KLD: 0.00005016, Weighted Average MSE: 11590.3232 / 10000 = 1.1590, Average CE: 2.1045\n",
      "Epoch 52, Average Loss: 3.2631, Average KLD: 0.00004619, Weighted Average MSE: 11590.2959 / 10000 = 1.1590, Average CE: 2.1040\n",
      "Epoch 53, Average Loss: 3.2637, Average KLD: 0.00004608, Weighted Average MSE: 11590.0322 / 10000 = 1.1590, Average CE: 2.1046\n",
      "Epoch 54, Average Loss: 3.2637, Average KLD: 0.00004212, Weighted Average MSE: 11589.9902 / 10000 = 1.1590, Average CE: 2.1047\n",
      "Epoch 55, Average Loss: 3.2651, Average KLD: 0.00004154, Weighted Average MSE: 11589.9463 / 10000 = 1.1590, Average CE: 2.1061\n",
      "Epoch 56, Average Loss: 3.2659, Average KLD: 0.00003779, Weighted Average MSE: 11589.8838 / 10000 = 1.1590, Average CE: 2.1069\n",
      "Epoch 57, Average Loss: 3.2631, Average KLD: 0.00004798, Weighted Average MSE: 11589.6367 / 10000 = 1.1590, Average CE: 2.1041\n",
      "Epoch 58, Average Loss: 3.2640, Average KLD: 0.00005418, Weighted Average MSE: 11589.4629 / 10000 = 1.1589, Average CE: 2.1050\n",
      "Epoch 59, Average Loss: 3.2634, Average KLD: 0.00005523, Weighted Average MSE: 11589.5303 / 10000 = 1.1590, Average CE: 2.1043\n",
      "Epoch 60, Average Loss: 3.2640, Average KLD: 0.00005430, Weighted Average MSE: 11589.5723 / 10000 = 1.1590, Average CE: 2.1050\n",
      "Epoch 61, Average Loss: 3.2627, Average KLD: 0.00005597, Weighted Average MSE: 11589.4658 / 10000 = 1.1589, Average CE: 2.1037\n",
      "Epoch 62, Average Loss: 3.2639, Average KLD: 0.00004784, Weighted Average MSE: 11589.3994 / 10000 = 1.1589, Average CE: 2.1050\n",
      "Epoch 63, Average Loss: 3.2638, Average KLD: 0.00003974, Weighted Average MSE: 11589.1006 / 10000 = 1.1589, Average CE: 2.1048\n",
      "Epoch 64, Average Loss: 3.2650, Average KLD: 0.00003689, Weighted Average MSE: 11589.0342 / 10000 = 1.1589, Average CE: 2.1060\n",
      "Epoch 65, Average Loss: 3.2634, Average KLD: 0.00004165, Weighted Average MSE: 11589.2119 / 10000 = 1.1589, Average CE: 2.1044\n",
      "Epoch 66, Average Loss: 3.2638, Average KLD: 0.00003778, Weighted Average MSE: 11588.8848 / 10000 = 1.1589, Average CE: 2.1048\n",
      "Epoch 67, Average Loss: 3.2641, Average KLD: 0.00004580, Weighted Average MSE: 11588.9170 / 10000 = 1.1589, Average CE: 2.1051\n",
      "Epoch 68, Average Loss: 3.2651, Average KLD: 0.00004347, Weighted Average MSE: 11588.9756 / 10000 = 1.1589, Average CE: 2.1061\n",
      "Epoch 69, Average Loss: 3.2624, Average KLD: 0.00004550, Weighted Average MSE: 11588.8359 / 10000 = 1.1589, Average CE: 2.1035\n",
      "Epoch 70, Average Loss: 3.2641, Average KLD: 0.00004345, Weighted Average MSE: 11588.8145 / 10000 = 1.1589, Average CE: 2.1051\n",
      "Epoch 71, Average Loss: 3.2647, Average KLD: 0.00004977, Weighted Average MSE: 11588.8799 / 10000 = 1.1589, Average CE: 2.1057\n",
      "Epoch 72, Average Loss: 3.2627, Average KLD: 0.00004721, Weighted Average MSE: 11588.8418 / 10000 = 1.1589, Average CE: 2.1038\n",
      "Epoch 73, Average Loss: 3.2635, Average KLD: 0.00005883, Weighted Average MSE: 11588.6816 / 10000 = 1.1589, Average CE: 2.1046\n",
      "Epoch 74, Average Loss: 3.2652, Average KLD: 0.00005377, Weighted Average MSE: 11588.6953 / 10000 = 1.1589, Average CE: 2.1063\n",
      "Epoch 75, Average Loss: 3.2644, Average KLD: 0.00005222, Weighted Average MSE: 11588.5615 / 10000 = 1.1589, Average CE: 2.1055\n",
      "Epoch 76, Average Loss: 3.2635, Average KLD: 0.00005166, Weighted Average MSE: 11588.6807 / 10000 = 1.1589, Average CE: 2.1046\n",
      "Epoch 77, Average Loss: 3.2635, Average KLD: 0.00005675, Weighted Average MSE: 11588.3428 / 10000 = 1.1588, Average CE: 2.1046\n",
      "Epoch 78, Average Loss: 3.2636, Average KLD: 0.00005538, Weighted Average MSE: 11588.5303 / 10000 = 1.1589, Average CE: 2.1047\n",
      "Epoch 79, Average Loss: 3.2633, Average KLD: 0.00005232, Weighted Average MSE: 11588.3965 / 10000 = 1.1588, Average CE: 2.1044\n",
      "Epoch 80, Average Loss: 3.2648, Average KLD: 0.00005283, Weighted Average MSE: 11588.5146 / 10000 = 1.1589, Average CE: 2.1059\n",
      "Epoch 81, Average Loss: 3.2635, Average KLD: 0.00005001, Weighted Average MSE: 11588.5342 / 10000 = 1.1589, Average CE: 2.1046\n",
      "Epoch 82, Average Loss: 3.2626, Average KLD: 0.00005036, Weighted Average MSE: 11588.1875 / 10000 = 1.1588, Average CE: 2.1037\n",
      "Epoch 83, Average Loss: 3.2641, Average KLD: 0.00004924, Weighted Average MSE: 11588.3594 / 10000 = 1.1588, Average CE: 2.1052\n",
      "Epoch 84, Average Loss: 3.2654, Average KLD: 0.00004367, Weighted Average MSE: 11588.2588 / 10000 = 1.1588, Average CE: 2.1065\n",
      "Epoch 85, Average Loss: 3.2640, Average KLD: 0.00005139, Weighted Average MSE: 11588.2764 / 10000 = 1.1588, Average CE: 2.1051\n",
      "Epoch 86, Average Loss: 3.2614, Average KLD: 0.00006357, Weighted Average MSE: 11588.0674 / 10000 = 1.1588, Average CE: 2.1025\n",
      "Epoch 87, Average Loss: 3.2652, Average KLD: 0.00006091, Weighted Average MSE: 11588.0674 / 10000 = 1.1588, Average CE: 2.1063\n",
      "Epoch 88, Average Loss: 3.2635, Average KLD: 0.00004175, Weighted Average MSE: 11588.3525 / 10000 = 1.1588, Average CE: 2.1046\n",
      "Epoch 89, Average Loss: 3.2627, Average KLD: 0.00004399, Weighted Average MSE: 11588.4316 / 10000 = 1.1588, Average CE: 2.1038\n",
      "Epoch 90, Average Loss: 3.2635, Average KLD: 0.00004817, Weighted Average MSE: 11588.1152 / 10000 = 1.1588, Average CE: 2.1046\n",
      "Epoch 91, Average Loss: 3.2646, Average KLD: 0.00004579, Weighted Average MSE: 11588.0654 / 10000 = 1.1588, Average CE: 2.1057\n",
      "Epoch 92, Average Loss: 3.2632, Average KLD: 0.00004288, Weighted Average MSE: 11588.1934 / 10000 = 1.1588, Average CE: 2.1044\n",
      "Epoch 93, Average Loss: 3.2643, Average KLD: 0.00003553, Weighted Average MSE: 11588.2578 / 10000 = 1.1588, Average CE: 2.1054\n",
      "Epoch 94, Average Loss: 3.2637, Average KLD: 0.00003883, Weighted Average MSE: 11587.8525 / 10000 = 1.1588, Average CE: 2.1049\n",
      "Epoch 95, Average Loss: 3.2627, Average KLD: 0.00004478, Weighted Average MSE: 11588.0371 / 10000 = 1.1588, Average CE: 2.1038\n",
      "Epoch 96, Average Loss: 3.2646, Average KLD: 0.00004781, Weighted Average MSE: 11588.1113 / 10000 = 1.1588, Average CE: 2.1057\n",
      "Epoch 97, Average Loss: 3.2643, Average KLD: 0.00004723, Weighted Average MSE: 11587.9951 / 10000 = 1.1588, Average CE: 2.1054\n",
      "Epoch 98, Average Loss: 3.2635, Average KLD: 0.00004830, Weighted Average MSE: 11588.0547 / 10000 = 1.1588, Average CE: 2.1046\n",
      "Epoch 99, Average Loss: 3.2636, Average KLD: 0.00005255, Weighted Average MSE: 11587.9375 / 10000 = 1.1588, Average CE: 2.1047\n",
      "Epoch 100, Average Loss: 3.2644, Average KLD: 0.00004182, Weighted Average MSE: 11588.0723 / 10000 = 1.1588, Average CE: 2.1055\n"
     ]
    }
   ],
   "source": [
    "supervised_vae_model = SupervisedVAE(input_channels=3, latent_dim=20, output_channels=3)\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "supervised_vae_model.to(device)\n",
    "train_supervised_vae(supervised_vae_model, train_loader, epochs=100, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1896462018730489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       129\n",
      "           1       0.00      0.00      0.00       112\n",
      "           2       0.00      0.00      0.00       107\n",
      "           3       0.00      0.00      0.00       123\n",
      "           4       0.00      0.00      0.00       123\n",
      "           5       0.20      0.33      0.25       723\n",
      "           6       0.18      0.42      0.26       681\n",
      "           7       0.19      0.27      0.22       693\n",
      "           8       0.00      0.00      0.00       109\n",
      "           9       0.00      0.00      0.00       363\n",
      "          10       0.15      0.02      0.04       681\n",
      "\n",
      "    accuracy                           0.19      3844\n",
      "   macro avg       0.07      0.10      0.07      3844\n",
      "weighted avg       0.13      0.19      0.14      3844\n",
      "\n",
      "Average Reconstruction Error: 0.9195633545394771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lione\\miniconda3\\envs\\IMED2024\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Lione\\miniconda3\\envs\\IMED2024\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Lione\\miniconda3\\envs\\IMED2024\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lione\\AppData\\Local\\Temp\\ipykernel_27732\\2312625152.py:25: UserWarning: Using a target size (torch.Size([3, 33, 4])) that is different to the input size (torch.Size([32, 3, 33, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_errors = [F.mse_loss(r, inputs).item() for r, inputs in zip(reconstructions, test_loader.dataset.x_data)]\n",
      "C:\\Users\\Lione\\AppData\\Local\\Temp\\ipykernel_27732\\2312625152.py:25: UserWarning: Using a target size (torch.Size([3, 33, 4])) that is different to the input size (torch.Size([4, 3, 33, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_errors = [F.mse_loss(r, inputs).item() for r, inputs in zip(reconstructions, test_loader.dataset.x_data)]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    reconstructions = []\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['spectrograms'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            recon, _, _, logits = model(inputs)\n",
    "            preds = torch.argmax(logits, dim=1)  # Get the class predictions\n",
    "            predictions.extend(preds.cpu().numpy())  # Store predictions\n",
    "            reconstructions.append(recon.cpu())  # Store reconstructions\n",
    "    return predictions, reconstructions\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming `test_loader` is your DataLoader for the test set\n",
    "predictions, reconstructions = predict(supervised_vae_model, test_loader, device)\n",
    "\n",
    "# Assuming `true_labels` are your actual labels for the test dataset\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# For reconstruction error, if applicable:\n",
    "recon_errors = [F.mse_loss(r, inputs).item() for r, inputs in zip(reconstructions, test_loader.dataset.x_data)]\n",
    "print(\"Average Reconstruction Error:\", np.mean(recon_errors))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_reconstructions(inputs, recons, n=10):\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     for i in range(n):\n",
    "#         plt.subplot(2, n, i + 1)\n",
    "#         plt.imshow(inputs[i].squeeze().numpy())  # Adjust based on your data\n",
    "#         plt.title(\"Original\")\n",
    "#         plt.axis('off')\n",
    "\n",
    "#         plt.subplot(2, n, n + i + 1)\n",
    "#         plt.imshow(recons[i].squeeze().numpy())  # Adjust based on your data\n",
    "#         plt.title(\"Reconstructed\")\n",
    "#         plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # Select first n examples to plot\n",
    "# plot_reconstructions(test_loader.dataset.x_data[:10], reconstructions[:10])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
